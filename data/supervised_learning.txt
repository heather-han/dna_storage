2.1 Introduction
The first three examples described in Chapter 1 have several components in common. For each there is a set of variables that might be denoted as inputs, which are measured or preset. These have some influence on one or more outputs. For each example the goal is to use the inputs to predict the values of the outputs. This exercise is called supervised learning.
We have used the more modern language of machine learning. In the statistical literature the inputs are often called the predictors, a term we will use interchangeably with inputs, and more classically the independent variables. In the pattern recognition literature the term features is preferred, which we use as well. The outputs are called the responses, or classically the dependent variables.
2.2 Variable Types and Terminology
The outputs vary in nature among the examples. In the glucose prediction example, the output is a quantitative measurement, where some measure- ments are bigger than others, and measurements close in value are close in nature. In the famous Iris discrimination example due to R. A. Fisher, the output is qualitative (species of Iris) and assumes values in a finite set G = {Virginica, Setosa and Versicolor}. In the handwritten digit example the output is one of 10 different digit classes: G = {0,1,...,9}. In both ofthese there is no explicit ordering in the classes, and in fact often descrip- tive labels rather than numbers are used to denote the classes. Qualitative variables are also referred to as categorical or discrete variables as well as factors.
For both types of outputs it makes sense to think of using the inputs to predict the output. Given some specific atmospheric measurements today and yesterday, we want to predict the ozone level tomorrow. Given the grayscale values for the pixels of the digitized image of the handwritten digit, we want to predict its class label.
This distinction in output type has led to a naming convention for the prediction tasks: regression when we predict quantitative outputs, and clas- sification when we predict qualitative outputs. We will see that these two tasks have a lot in common, and in particular both can be viewed as a task in function approximation.
Inputs also vary in measurement type; we can have some of each of qual- itative and quantitative input variables. These have also led to distinctions in the types of methods that are used for prediction: some methods are defined most naturally for quantitative inputs, some most naturally for qualitative and some for both.
A third variable type is ordered categorical, such as small, medium and large, where there is an ordering between the values, but no metric notion is appropriate (the difference between medium and small need not be the same as that between large and medium). These are discussed further in Chapter 4.
Qualitative variables are typically represented numerically by codes. The easiest case is when there are only two classes or categories, such as “suc- cess” or “failure,” “survived” or “died.” These are often represented by a single binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will become apparent, such numeric codes are sometimes referred to as targets. When there are more than two categories, several alternatives are available. The most useful and commonly used coding is via dummy variables. Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is “on” at a time. Although more compact coding schemes are possible, dummy variables are symmetric in the levels of the factor.
We will typically denote an input variable by the symbol X. If X is a vector, its components can be accessed by subscripts Xj. Quantitative outputs will be denoted by Y , and qualitative outputs by G (for group). We use uppercase letters such as X, Y or G when referring to the generic aspects of a variable. Observed values are written in lowercase; hence the ith observed value of X is written as xi (where xi is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of N input p-vectors xi, i = 1,...,N would be represented by the N×p matrix X. In general, vectors will not be bold, except when they have N components; this convention distinguishes a p-vector of inputs xi for the ith observation from the N-vector xj consisting of all the observations on variable Xj. Since all vectors are assumed to be column vectors, the ith row of X is xTi , the vector transpose of xi.
For the moment we can loosely state the learning task as follows: given the value of an input vector X, make a good prediction of the output Y, denoted by Yˆ (pronounced “y-hat”). If Y takes values in IR then so should Yˆ ; likewise for categorical outputs, Gˆ should take values in the same set G associated with G.
For a two-class G, one approach is to denote the binary coded target as Y , and then treat it as a quantitative output. The predictions Yˆ will typically lie in [0,1], and we can assign to Gˆ the class label according to whether yˆ > 0.5. This approach generalizes to K-level qualitative outputs as well.
We need data to construct prediction rules, often a lot of it. We thus suppose we have available a set of measurements (xi, yi) or (xi, gi), i = 1, . . . , N , known as the training data, with which to construct our prediction rule.